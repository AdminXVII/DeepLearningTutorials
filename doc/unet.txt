.. _unet:

U-Net
**********************************************

.. note::
    This section assumes the reader has already read through :doc:`lenet` for
    convolutional networks motivation and :doc:`fcn_2D_segm` for segmentation
    network.

Summary
+++++++

This tutorial provides a brief explanation of the U-Net architecture as well as a way to implement
it using Theano and Lasagne. U-Net is a Fully Convolutional Network (FCN) that does image segmentation.
Its goal is then to predict each pixel's class.
Compared to image classification, the difficulty arises from the fact that localisation is important.
The network must capture the overall context and recognize very precise details.


Model
+++++

The U-Net architecture is built upon the Fully Convolutional Network and modified
in a way that it yields better segmentation in medical imaging.
Compared to FCN-8, the two main differences are (1) U-net is symmetric and (2) the skip
connections between the downsampling path and the upsampling path apply a concatenation
operator instead of a sum. These skip connections intend to provide local information
to the global information while upsampling.
Because of its symmetry, the network has a large number of feature maps in the upsampling
path, which allows to transfer information. By comparison, the basic FCN architecture only had
*number of classes* feature maps in its upsampling path.

The U-Net owes its name to its symmetric shape, which is different from other FCN variants.

U-Net architecture is separated in 3 parts:

- 1 : The contracting/downsampling path
- 2: Bottleneck
- 2 : The expanding/upsampling path

.. figure:: images/unet.jpg
    :align: center
    :scale: 60%

    **Figure 1** : Illustration of U-Net architecture (from U-Net paper)


Contracting/downsampling path
=============================

The contracting path is composed of 4 blocks. Each block is composed of

* 3x3 Convolution Layer + activation function
* 3x3 Convolution Layer + activation function
* 2x2 Max Pooling

Note that the number of feature maps doubles at each pooling, starting with
64 feature maps for the first block, 128 for the second, and so on.
The purpose of this contracting path is to capture the context of the input image
in order to be able to do segmentation. This coarse contextual information will
then be transfered to the upsampling path by means of skip connections.


Bottleneck
==========

This part of the network is between the contracting and expanding paths.
The bottleneck is built from simply 2 convolutional layers, with dropout.


Expanding/upsampling path
=========================

The expanding path is also composed of 4 blocks. Each of these blocks is composed of

* Deconvolution layer with stride 2
* Concatenation with the corresponding cropped feature map from the contracting path
* 3x3 Convolution layer + activation function
* 3x3 Convolution layer + activation function


The purpose of this expanding path is to enable precise localization combined
with contextual information from the contracting path.

Advantages
==========

* The U-Net combines the location information from the downsampling path with the contextual information in the upsampling path to finally obtain a general information combining localisation and context, which is necessary to predict a good segmentation map.

* No dense layer, so images of different sizes can be used as input (since the only parameters to learn on convolution layers are the kernel, and the size of the kernel is independent from input image' size).

* The use of massive data augmentation is important in domains like biomedical segmentation, since the number of annotated samples is usually limited.


Code - Citations - Contact
++++++++++++++++++++++++++

Code
====

The U-Net implementation can be found in the following GitHub repo:

* `Unet.py <https://github.com/Lasagne/Recipes/blob/master/modelzoo/Unet.py>`_ : Main script. Defines the model

The user can now build a U-Net with a specified number of input channels and number of classes.

First include the Lasagne layers needed to define the U-Net architecture :

.. literalinclude:: ../code/Unet_lasagne_recipes.py
  :start-after: start-snippet-1
  :end-before: end-snippet-1

The *net* variable will be an ordered dictionary containing the layers' name as key and the layer instance as value.
This is needed to be able to concatenate the feature maps from the contracting to expanding path.


First the contracting path :

.. literalinclude:: ../code/Unet_lasagne_recipes.py
  :start-after: start-snippet-downsampling
  :end-before: end-snippet-downsampling

And then the bottleneck :

.. literalinclude:: ../code/Unet_lasagne_recipes.py
  :start-after: start-snippet-bottleneck
  :end-before: end-snippet-bottleneck

Followed by the expanding path :

.. literalinclude:: ../code/Unet_lasagne_recipes.py
  :start-after: start-snippet-upsampling
  :end-before: end-snippet-upsampling

And finally the output path (to obtain *number of classes* feature maps):

.. literalinclude:: ../code/Unet_lasagne_recipes.py
  :start-after: start-snippet-output
  :end-before: end-snippet-output


Papers
======

If you use this tutorial, please cite the following papers.

U_Net: Convolutional Networks for Biomedical Image Segmentation

* `[pdf] <https://arxiv.org/pdf/1505.04597.pdf>`__ reference

Papers related to Theano:

* `[pdf] <http://www.iro.umontreal.ca/~lisa/pointeurs/nips2012_deep_workshop_theano_final.pdf>`__ Bastien, Frédéric, Lamblin, Pascal, Pascanu, Razvan, Bergstra, James, Goodfellow, Ian, Bergeron, Arnaud, Bouchard, Nicolas, and Bengio, Yoshua. Theano: new features and speed improvements. NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2012.

* `[pdf] <http://www.iro.umontreal.ca/~lisa/pointeurs/theano_scipy2010.pdf>`__ Bergstra, James, Breuleux, Olivier, Bastien, Frédéric, Lamblin, Pascal, Pascanu, Razvan, Desjardins, Guillaume, Turian, Joseph, Warde-Farley, David, and Bengio, Yoshua. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy), June 2010.

Thank you!

Contact
=======

Please email

References
++++++++++

* ref1

* ref2
