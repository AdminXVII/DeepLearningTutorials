.. _fcn_2D_segm:

Fully Convolutional Networks (FCN) for 2D segmentation
******************************************************

Summary
+++++++

Segmentation task is different from classification task because it require predicting
a class for each pixel of the input image, instead of only 1 class for the whole input.
Classification needs to understand *what* is in the input (namely, the context). However,
in order to predict what is in the input for each pixel, segmentation needs to recover
*what* is in the input, and *where*.

.. figure:: images/cat_segmentation.png
    :align: center
    :scale: 35%

    **Figure 1** : Segmentation network

TODO : reference de l'image

The **fully convolutional** network (FCN) owes its name to its architecture that
have only locally connected layers, such as convolution, pooling, upsampling and
no dense layer. It reduce the number of parameters and computation time. To obtain
its segmentation map (output), segmentation networks usually have 2 parts :

*  Convolution path : extract semantic/context information
*  Deconvolution path : recover spatial information

The **convolution path** is used to figure out and interpret the context, while the
**deconvolution path** is used to retrieve *where* in the image were detected the things
detected by the convolution path. Furthermore, to fully recover the spatial
information lost in the pooling or downsampling layers, we often use skip connections.

A skip connection is a connection that skips a least one layer. Here, it
is often used to transfer local information by concatenating or summing feature
maps from the convolution path
with feature maps from the deconvolution path. It helps combining context
information with spatial information.


Data
++++

Polyps

The polyps dataset can be found `[here] <https://drive.google.com/file/d/0B_60jvsCt1hhZWNfcW4wbHE5N3M/view>`__.
In each of the training, validation and test data, the input images are in the
/images directory and the polyps mask (segmentation map) are in /masks2. The
segmentation maps in the *masks2* directory indicate the presence or absence
of polyps for each pixel. The other subdirectories (/masks3 and /masks4) are,
respectively, for a segmentation task with 3 and 4 classes, but will not be
presented here.   


Model
+++++

The architecture for FCN network depends on the precision desired. The Figures
below show 3 different architectures : FCN32, FCN16 and FCN8. The convolutional
layers are represented as vertical lines between the pooling layers.
Those pooling layers explicitely show the relative size of the feature maps.

.. figure:: images/fcn.png
    :align: center
    :scale: 50%

    **Figure 2** : FCN architecture

**Difference between those 3 architectures**

These 3 different architectures differ in the stride for the last convolution,
and in the skip connections used to obtain their segmentation map, as you can
see in the image below. I will use the name *convolution path* for the network
up to *pool5*. Note that these 3 architectures have the same convolution path,
but their respective deconvolution path differ.


1. **FCN-32** : Directly produce the segmentation map from *pool5* by using a
deconvolution layer with stride 32.

2. **FCN-16** : Sum the 2x upsampled prediction from *pool5* with *pool4* to further
produce the segmentation map using a deconvolution layer with stride 16.

3. **FCN-8** : Sum the feature map obtained by summing *pool4* with the upsampled
*pool5* with *pool3*, and use a deconvolution with stride 8 on that feature map
to obtain the segmentation map.


.. figure:: images/fcn_schema.png
    :align: center
    :scale: 65%

    **Figure 3** : FCN architecture

As explained above, the deconvolution path is different, since it uses different
skip connection layers and different stride for the last convolution. It thus
yield different segmentation, as you can see in Figure 4 below. Combining layers
that have different precision helps retrieving fine and spatial information, as
well as coarse and context information.



.. figure:: images/fcn32_16_8.png
    :align: center
    :scale: 30%

    **Figure 4** : FCN results

Note that the FCN-8 architecture was used on the polyps dataset,
because it produces more precise segmentation map.


Metrics
=======

**Per pixel accuracy**

This metric is self explanatory, since it outputs the class prediction accuracy
per pixel.

.. math::
   :label: jaccard

    acc(P, GT) = \frac{|\text{pixels correctly predicted}|}{|\text{total nb of pixels}|}


**Jaccard (Intersection over Union)**

This evaluation metric is often used for image segmentation, since it is more structured.
The jaccard is a per class evaluation metric, which compute the nb of pixels in
the intersection between the
predicted and ground truth segmentation maps for a specified class, divided by the
number of pixels in the union between those two segmentation maps,
also for that specified class.

.. math::
   :label: jaccard

    jacc(P(class), GT(class)) = \frac{|P(class)\cap GT(class)|}{|P(class)\cup GT(class)|}

where :math:`P` is the predicted segmentation map and :math: `GT` is the ground
truth segmentation map. Often, a class is well segmented if its respective jaccard
is at least 0.5. In the polyps dataset, the jaccard(polyps) must thus be at
least 0.5.

Code - Citations - Contact
++++++++++++++++++++++++++

Code
====

The FCN8 implementation can be found in the following file:

* `fcn8.py <http://deeplearning.net/tutorial/code/fcn8.py>`_ : Defines the model.
* `train_fcn8.py <http://deeplearning.net/tutorial/code/train_fcn8.py>`_ : Training loop.


TODO : import model_helpers, dataset_loader, metrics



Papers
======

If you use this tutorial, please cite the following papers.

Fully Convolutional Networks for Semantic Segmentation

* `[pdf] <https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf>`__ reference

A Benchmark for Endoluminal Scene Segmentation of Colonoscopy Images

* `[pdf] <https://arxiv.org/pdf/1612.00799.pdf>`__ reference

Papers related to Theano:

* `[pdf] <http://www.iro.umontreal.ca/~lisa/pointeurs/nips2012_deep_workshop_theano_final.pdf>`__ Bastien, Frédéric, Lamblin, Pascal, Pascanu, Razvan, Bergstra, James, Goodfellow, Ian, Bergeron, Arnaud, Bouchard, Nicolas, and Bengio, Yoshua. Theano: new features and speed improvements. NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2012.

* `[pdf] <http://www.iro.umontreal.ca/~lisa/pointeurs/theano_scipy2010.pdf>`__ Bergstra, James, Breuleux, Olivier, Bastien, Frédéric, Lamblin, Pascal, Pascanu, Razvan, Desjardins, Guillaume, Turian, Joseph, Warde-Farley, David, and Bengio, Yoshua. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy), June 2010.

Thank you!

Contact
=======

Please email

References
++++++++++

* ref1

* ref2
